What other data processing can you do to create analytic data stores?
etl.py reads data from S3, processes that data using Spark, and writes them back to S3
dl.cfgcontains your AWS credentials
README.md provides discussion on your process and decisions
Document Process
Do the following steps in your README.md file.

Discuss the purpose of this database in context of the startup, Sparkify, and their analytical goals.
State and justify your schema design and ELT pipeline.
[Optional] Provide example queries and results for song play analysis.
Here's a guide on Markdown Syntax.
In your readme file, describe other data processing steps you took, like looking for data skewness, malformed data, or Map Reduce techniques.


# Steps:
1. Load AWS configuration parameters eg. 

# References:

- [Rubric](https://review.udacity.com/#!/rubrics/2502/view)