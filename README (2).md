# Data Modeling with Postgres

## **Overview**
A startup wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. 
Currently, they are collecting data in json format and the analytics team is particularly interested in understanding what songs users are listening to.

## **Technique**
In this project, we build an ETL pipeline using PySpark on AWS EMR Cluster 
from the input JSON format datasets stored in S3 : 
1. Song Dataset
2. Log Dataset

We create ETL program to create a temporary view table using PySpark dataframe, do processing and save back the temporary view into S3 in parquet format.

## Steps:
1. Set AWS paraemters in ``dl.cfg`` file 
2. Load and set the **AWS Parameteres** as OS environment variable 
3. Launch the `AWS EMR Cluster`
4. Create `Spark Session`

### Process Song Data:

5. Extract Song data from S3 and save it in temp view `song_data_table`
6. Run sql two separate SELECT queries to extract columns and create respective dataframes `songs_table` & `artists_table` 
7. write `songs_table` to parquet files partitioned by year and artist
8. write `artists_table` to parquet files

### Process Log Data:

9. Get filepath to log data file
10. read `log_data` file
11. filter by actions `nextsong` for song plays
12. created log view to write SQL Queries
13. extract columns for `users_table`    
14. write users table to parquet files
15. extract columns to create `time_table`
16. Write time table to parquet files `partitioned by year and month`

17. read in song data to use for `songplays_table`
18. extract columns from joined song and log datasets to create songplays_table 
19. write songplays_table to parquet files partitioned by year and month

## **Song Dataset**
Songs dataset is a subset of [Million Song Dataset](http://millionsongdataset.com/).

Sample Record :
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

## **Log Dataset**
Logs dataset is generated by [Event Simulator](https://github.com/Interana/eventsim).

Sample Record :
```
{"artist": null, "auth": "Logged In", "firstName": "Walter", "gender": "M", "itemInSession": 0, "lastName": "Frye", "length": null, "level": "free", "location": "San Francisco-Oakland-Hayward, CA", "method": "GET","page": "Home", "registration": 1540919166796.0, "sessionId": 38, "song": null, "status": 200, "ts": 1541105830796, "userAgent": "\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"", "userId": "39"}
```


## Schema


#### Fact Table 
**songplays** - records in log data associated with song plays i.e. records with page `NextSong`

```
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
```

#### Dimension Tables
**users**  - users in the app
```
user_id, first_name, last_name, gender, level
```
**songs**  - songs in music database
```
song_id, title, artist_id, year, duration
```
**artists**  - artists in music database
```
artist_id, name, location, latitude, longitude
```
**time**  - timestamps of records in  **songplays**  broken down into specific units
```
start_time, hour, day, week, month, year, weekday
```


## Project Files

```etl.py``` -> reads data **song_data** and **log_data** from S3, processes that data using Spark, and writes them back to S3

```dl.cfg``` -> contains your AWS credentials

```README.md``` -> provides discussion on your process and decisions

## Environment 
- Python 3.6.3 or above
- spark-2.4.3-bin-hadoop2.7
- AWS EMR Cluster
- AWS S3

## How to run

Run the program ```etl.py``` as below.
```
python etl.py 
```


 #### Reference: 

[PEP8 Documentation](https://www.python.org/dev/peps/pep-0008/)

[Python Docstring](https://www.datacamp.com/community/tutorials/docstrings-python)
